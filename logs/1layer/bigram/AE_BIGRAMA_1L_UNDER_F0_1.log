[2017-10-02 15:25:35,679 AE_BIGRAMA_1L_UNDER_F0_1.py:156]: >> Initializing execution of experiment AE_BIGRAMA_1L_UNDER_F0_1
[2017-10-02 15:25:35,679 AE_BIGRAMA_1L_UNDER_F0_1.py:157]: >> Printing header log
[2017-10-02 15:25:35,680 AE_BIGRAMA_1L_UNDER_F0_1.py:48]: 
	=======================================
	network_name = AE_BIGRAMA_1L_UNDER_F0_1
	layers = 9216,921
	using GLOBAL obj = 
		{'executed_dir': 'E:/research/research_msc/executed/onelayer/bigram/', 'mlp_configs': {'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x0000000001964588>, 'loss_function': 'categorical_crossentropy'}, 'autoencoder_configs': {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x00000000019606D8>, 'hidden_layer_activation': 'relu'}, 'tensorflow_dir': 'E:/research/research_msc/tensorflow/onelayer/bigram/', 'store_history': True, 'executed_path': 'E:/research/research_msc/executed/onelayer/bigram/', 'checkpoints_dir': 'E:/research/research_msc/checkpoints/onelayer/bigram/', 'log_format': '[%(asctime)s %(filename)s:%(lineno)s]: %(message)s', 'batch': 32, 'epochs': 200, 'numpy_seed': 666, 'log_dir': 'E:/research/research_msc/logs/onelayer/bigram/', 'data_target_list': [1, 2, 3, 4, 5, 6, 7, 8, 9], 'shuffle_batches': True, 'data_dir': 'E:/research/malware_dataset/malware_selected_2gram_mini.pkl', 'reports_dir': 'E:/research/research_msc/reports/onelayer/bigram/'}
	=======================================
	
[2017-10-02 15:25:35,680 AE_BIGRAMA_1L_UNDER_F0_1.py:159]: >> Loading dataset... 
[2017-10-02 15:25:36,092 AE_BIGRAMA_1L_UNDER_F0_1.py:64]: 
	=======================================
	loading malware dataset on = E:/research/malware_dataset/malware_selected_2gram_mini.pkl	
	trainx shape = (1627, 9216)
	trainy shape = (1627, 9)
	valx shape = (1076, 9216)
	valy shape = (1076, 9)
	=======================================
	
[2017-10-02 15:25:36,092 AE_BIGRAMA_1L_UNDER_F0_1.py:161]: >> Executing autoencoder part ... 
[2017-10-02 15:25:36,092 AE_BIGRAMA_1L_UNDER_F0_1.py:69]: =======================================
[2017-10-02 15:25:36,092 AE_BIGRAMA_1L_UNDER_F0_1.py:74]: setting configurations for autoencoder: 
	 {'discard_decoder_function': True, 'loss_function': 'mse', 'output_layer_activation': 'relu', 'optimizer': <keras.optimizers.SGD object at 0x00000000019606D8>, 'hidden_layer_activation': 'relu'}
[2017-10-02 15:25:36,149 AE_BIGRAMA_1L_UNDER_F0_1.py:85]: training and evaluate autoencoder
[2017-10-02 15:25:36,725 summary.py:93]: Summary name enc0_921/kernel:0 is illegal; using enc0_921/kernel_0 instead.
[2017-10-02 15:25:36,727 summary.py:93]: Summary name enc0_921/bias:0 is illegal; using enc0_921/bias_0 instead.
[2017-10-02 15:25:36,731 summary.py:93]: Summary name dec0_9216/kernel:0 is illegal; using dec0_9216/kernel_0 instead.
[2017-10-02 15:25:36,733 summary.py:93]: Summary name dec0_9216/bias:0 is illegal; using dec0_9216/bias_0 instead.
[2017-10-02 15:43:07,212 AE_BIGRAMA_1L_UNDER_F0_1.py:96]: trained and evaluated!
[2017-10-02 15:43:07,212 AE_BIGRAMA_1L_UNDER_F0_1.py:99]: Training history: 
{'val_loss': [0.00010817516280284504, 0.00010817084653708966, 0.00010816650235762328, 0.00010816212555806441, 0.00010815770883540723, 0.00010815325438055352, 0.00010814876635892137, 0.00010814425361526223, 0.00010813969910922928, 0.00010813510335473773, 0.00010813048582646968, 0.00010812585712730761, 0.00010812119478022257, 0.00010811651198472135, 0.00010811180533273458, 0.00010810707842166878, 0.00010810232002653361, 0.00010809753450208438, 0.00010809272879970062, 0.00010808790383902014, 0.00010808305788896008, 0.00010807816560538554, 0.00010807325950019636, 0.00010806832940328077, 0.00010806337174428038, 0.00010805839563842835, 0.00010805338258477669, 0.00010804836660992271, 0.00010804333126857932, 0.00010803830047132843, 0.00010803323167139927, 0.00010802814320745085, 0.00010802304119236947, 0.0001080179268162746, 0.00010801279583260362, 0.0001080076480790675, 0.0001080024884784331, 0.00010799730780727463, 0.00010799211669552268, 0.00010798690294445282, 0.00010798168286411127, 0.00010797646059286797, 0.00010797123661759001, 0.00010796600896376098, 0.00010796076578428263, 0.00010795549098549403, 0.00010795019443997712, 0.00010794487782471837, 0.00010793955182374479, 0.00010793420661857082, 0.00010792884323702689, 0.00010792346414049644], 'loss': [0.00010849936590975033, 0.00010849505051558771, 0.00010849070393363874, 0.00010848632518453357, 0.00010848192229105523, 0.00010847748040613595, 0.00010847299685998669, 0.00010846847332961063, 0.00010846391751133416, 0.00010845933912283143, 0.00010845475091379835, 0.00010845014415487455, 0.00010844551381505064, 0.00010844085120521434, 0.00010843616843556433, 0.00010843146097148409, 0.00010842673849487178, 0.00010842198034952072, 0.0001084172076835585, 0.00010841241800160443, 0.00010840760549899167, 0.00010840275629460623, 0.00010839787303140506, 0.00010839296485911729, 0.00010838803807433068, 0.00010838309411255994, 0.00010837811020681046, 0.00010837310267544077, 0.00010836808834214636, 0.00010836306720245521, 0.00010835802639934326, 0.00010835294673447861, 0.00010834784686955216, 0.00010834273593193297, 0.00010833761296013929, 0.00010833247346874669, 0.00010832731286500258, 0.0001083221250669756, 0.00010831692442086857, 0.00010831170924520635, 0.00010830647518872481, 0.00010830123082426398, 0.00010829598978697737, 0.00010829073589266672, 0.00010828547362228437, 0.00010828017993604327, 0.00010827485419891817, 0.00010826950267619283, 0.00010826412983987562, 0.00010825875135988384, 0.00010825335226840549, 0.00010824793009689197]}
[2017-10-02 15:43:07,213 AE_BIGRAMA_1L_UNDER_F0_1.py:103]: done!
[2017-10-02 15:43:07,213 AE_BIGRAMA_1L_UNDER_F0_1.py:163]: >> Executing classifier part ... 
[2017-10-02 15:43:07,213 AE_BIGRAMA_1L_UNDER_F0_1.py:108]: =======================================
[2017-10-02 15:43:07,213 AE_BIGRAMA_1L_UNDER_F0_1.py:112]: setting configurations for classifier: 
	 {'classifier_dim': 9, 'use_last_dim_as_classifier': False, 'activation': 'sigmoid', 'optimizer': <keras.optimizers.SGD object at 0x0000000001964588>, 'loss_function': 'categorical_crossentropy'}
[2017-10-02 15:43:07,274 AE_BIGRAMA_1L_UNDER_F0_1.py:121]: training ... 
[2017-10-02 15:43:07,748 summary.py:93]: Summary name enc0_921/kernel:0 is illegal; using enc0_921/kernel_0 instead.
[2017-10-02 15:43:07,749 summary.py:93]: Summary name enc0_921/bias:0 is illegal; using enc0_921/bias_0 instead.
[2017-10-02 15:43:07,753 summary.py:93]: Summary name classifier/kernel:0 is illegal; using classifier/kernel_0 instead.
[2017-10-02 15:43:07,754 summary.py:93]: Summary name classifier/bias:0 is illegal; using classifier/bias_0 instead.
[2017-10-02 15:50:14,766 AE_BIGRAMA_1L_UNDER_F0_1.py:133]: trained!
[2017-10-02 15:50:14,767 AE_BIGRAMA_1L_UNDER_F0_1.py:136]: Training history: 
{'val_loss': [0.00010817516280284504, 0.00010817084653708966, 0.00010816650235762328, 0.00010816212555806441, 0.00010815770883540723, 0.00010815325438055352, 0.00010814876635892137, 0.00010814425361526223, 0.00010813969910922928, 0.00010813510335473773, 0.00010813048582646968, 0.00010812585712730761, 0.00010812119478022257, 0.00010811651198472135, 0.00010811180533273458, 0.00010810707842166878, 0.00010810232002653361, 0.00010809753450208438, 0.00010809272879970062, 0.00010808790383902014, 0.00010808305788896008, 0.00010807816560538554, 0.00010807325950019636, 0.00010806832940328077, 0.00010806337174428038, 0.00010805839563842835, 0.00010805338258477669, 0.00010804836660992271, 0.00010804333126857932, 0.00010803830047132843, 0.00010803323167139927, 0.00010802814320745085, 0.00010802304119236947, 0.0001080179268162746, 0.00010801279583260362, 0.0001080076480790675, 0.0001080024884784331, 0.00010799730780727463, 0.00010799211669552268, 0.00010798690294445282, 0.00010798168286411127, 0.00010797646059286797, 0.00010797123661759001, 0.00010796600896376098, 0.00010796076578428263, 0.00010795549098549403, 0.00010795019443997712, 0.00010794487782471837, 0.00010793955182374479, 0.00010793420661857082, 0.00010792884323702689, 0.00010792346414049644], 'loss': [0.00010849936590975033, 0.00010849505051558771, 0.00010849070393363874, 0.00010848632518453357, 0.00010848192229105523, 0.00010847748040613595, 0.00010847299685998669, 0.00010846847332961063, 0.00010846391751133416, 0.00010845933912283143, 0.00010845475091379835, 0.00010845014415487455, 0.00010844551381505064, 0.00010844085120521434, 0.00010843616843556433, 0.00010843146097148409, 0.00010842673849487178, 0.00010842198034952072, 0.0001084172076835585, 0.00010841241800160443, 0.00010840760549899167, 0.00010840275629460623, 0.00010839787303140506, 0.00010839296485911729, 0.00010838803807433068, 0.00010838309411255994, 0.00010837811020681046, 0.00010837310267544077, 0.00010836808834214636, 0.00010836306720245521, 0.00010835802639934326, 0.00010835294673447861, 0.00010834784686955216, 0.00010834273593193297, 0.00010833761296013929, 0.00010833247346874669, 0.00010832731286500258, 0.0001083221250669756, 0.00010831692442086857, 0.00010831170924520635, 0.00010830647518872481, 0.00010830123082426398, 0.00010829598978697737, 0.00010829073589266672, 0.00010828547362228437, 0.00010828017993604327, 0.00010827485419891817, 0.00010826950267619283, 0.00010826412983987562, 0.00010825875135988384, 0.00010825335226840549, 0.00010824793009689197]}
[2017-10-02 15:50:14,767 AE_BIGRAMA_1L_UNDER_F0_1.py:140]: evaluating model ... 
[2017-10-02 15:50:15,280 AE_BIGRAMA_1L_UNDER_F0_1.py:144]: evaluated! 
[2017-10-02 15:50:15,280 AE_BIGRAMA_1L_UNDER_F0_1.py:146]: generating reports ... 
[2017-10-02 15:50:15,752 AE_BIGRAMA_1L_UNDER_F0_1.py:149]: done!
[2017-10-02 15:50:15,753 AE_BIGRAMA_1L_UNDER_F0_1.py:165]: >> experiment AE_BIGRAMA_1L_UNDER_F0_1 finished!
